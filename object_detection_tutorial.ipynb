{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-yl-s-WKMG"
   },
   "source": [
    "# Object Detection Demo\n",
    "Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFSqkTCdWKMI"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "import pprint\n",
    "\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\".\")\n",
    "#from object_detection.utils import ops as utils_ops\n",
    "\n",
    "#if StrictVersion(tf.__version__) < StrictVersion('1.12.0'):\n",
    "#    raise ImportError('Please upgrade your TensorFlow installation to v1.12.*.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfn_tRFOWKMO"
   },
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference configs\n",
    "IMAGE_PATH = '/home/johann/Pictures/images_compl'\n",
    "DESTINATION_PATH = '/home/johann/Pictures/dest_images'\n",
    "IMAGE_DIMS = [500, 500]\n",
    "FIGURE_DIMS = tuple(IMAGE_DIMS.copy())\n",
    "BATCH_NUM = 20\n",
    "THRESHOLD = 20\n",
    "\n",
    "# What model to download.\n",
    "MODEL_NAME = 'ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "INFERENCE_GRAPH_PATH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "#Download url\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "VISUALIZE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KILYnwR5WKMS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file already exists\n",
      "Model file already extracted\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(MODEL_FILE):\n",
    "    print(\"Downloading model file\")\n",
    "    opener = urllib.request.URLopener()\n",
    "    opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "else:\n",
    "    print(\"Model file already exists\")\n",
    "    \n",
    "if not os.path.isfile(INFERENCE_GRAPH_PATH):\n",
    "    tar_file = tarfile.open(MODEL_FILE)\n",
    "    for file in tar_file.getmembers():\n",
    "        file_name = os.path.basename(file.name)\n",
    "        if 'frozen_inference_graph.pb' in file_name:\n",
    "            tar_file.extract(file, os.getcwd())\n",
    "else:\n",
    "    print('Model file already extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MVVTcLWKMW"
   },
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_INDEX = {}\n",
    "f = open(\"object_detection/mscoco_label_map.pbtxt\", \"r\")\n",
    "for line in f:\n",
    "    if 'item' in line:\n",
    "        name = f.readline().split()[1]\n",
    "        real_id = int(f.readline().split()[1])\n",
    "        real_name = f.readline().split()[1]\n",
    "        CATEGORY_INDEX[real_id] =  {'id': real_id, 'name': real_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'id': 1, 'name': 'person'}, 2: {'id': 2, 'name': 'bicycle'}, 3: {'id': 3, 'name': 'car'}, 4: {'id': 4, 'name': 'motorcycle'}, 5: {'id': 5, 'name': 'airplane'}, 6: {'id': 6, 'name': 'bus'}, 7: {'id': 7, 'name': 'train'}, 8: {'id': 8, 'name': 'truck'}, 9: {'id': 9, 'name': 'boat'}, 10: {'id': 10, 'name': 'traffic'}, 11: {'id': 11, 'name': 'fire'}, 13: {'id': 13, 'name': 'stop'}, 14: {'id': 14, 'name': 'parking'}, 15: {'id': 15, 'name': 'bench'}, 16: {'id': 16, 'name': 'bird'}, 17: {'id': 17, 'name': 'cat'}, 18: {'id': 18, 'name': 'dog'}, 19: {'id': 19, 'name': 'horse'}, 20: {'id': 20, 'name': 'sheep'}, 21: {'id': 21, 'name': 'cow'}, 22: {'id': 22, 'name': 'elephant'}, 23: {'id': 23, 'name': 'bear'}, 24: {'id': 24, 'name': 'zebra'}, 25: {'id': 25, 'name': 'giraffe'}, 27: {'id': 27, 'name': 'backpack'}, 28: {'id': 28, 'name': 'umbrella'}, 31: {'id': 31, 'name': 'handbag'}, 32: {'id': 32, 'name': 'tie'}, 33: {'id': 33, 'name': 'suitcase'}, 34: {'id': 34, 'name': 'frisbee'}, 35: {'id': 35, 'name': 'skis'}, 36: {'id': 36, 'name': 'snowboard'}, 37: {'id': 37, 'name': 'sports'}, 38: {'id': 38, 'name': 'kite'}, 39: {'id': 39, 'name': 'baseball'}, 40: {'id': 40, 'name': 'baseball'}, 41: {'id': 41, 'name': 'skateboard'}, 42: {'id': 42, 'name': 'surfboard'}, 43: {'id': 43, 'name': 'tennis'}, 44: {'id': 44, 'name': 'bottle'}, 46: {'id': 46, 'name': 'wine'}, 47: {'id': 47, 'name': 'cup'}, 48: {'id': 48, 'name': 'fork'}, 49: {'id': 49, 'name': 'knife'}, 50: {'id': 50, 'name': 'spoon'}, 51: {'id': 51, 'name': 'bowl'}, 52: {'id': 52, 'name': 'banana'}, 53: {'id': 53, 'name': 'apple'}, 54: {'id': 54, 'name': 'sandwich'}, 55: {'id': 55, 'name': 'orange'}, 56: {'id': 56, 'name': 'broccoli'}, 57: {'id': 57, 'name': 'carrot'}, 58: {'id': 58, 'name': 'hot'}, 59: {'id': 59, 'name': 'pizza'}, 60: {'id': 60, 'name': 'donut'}, 61: {'id': 61, 'name': 'cake'}, 62: {'id': 62, 'name': 'chair'}, 63: {'id': 63, 'name': 'couch'}, 64: {'id': 64, 'name': 'potted'}, 65: {'id': 65, 'name': 'bed'}, 67: {'id': 67, 'name': 'dining'}, 70: {'id': 70, 'name': 'toilet'}, 72: {'id': 72, 'name': 'tv'}, 73: {'id': 73, 'name': 'laptop'}, 74: {'id': 74, 'name': 'mouse'}, 75: {'id': 75, 'name': 'remote'}, 76: {'id': 76, 'name': 'keyboard'}, 77: {'id': 77, 'name': 'cell'}, 78: {'id': 78, 'name': 'microwave'}, 79: {'id': 79, 'name': 'oven'}, 80: {'id': 80, 'name': 'toaster'}, 81: {'id': 81, 'name': 'sink'}, 82: {'id': 82, 'name': 'refrigerator'}, 84: {'id': 84, 'name': 'book'}, 85: {'id': 85, 'name': 'clock'}, 86: {'id': 86, 'name': 'vase'}, 87: {'id': 87, 'name': 'scissors'}, 88: {'id': 88, 'name': 'teddy'}, 89: {'id': 89, 'name': 'hair'}, 90: {'id': 90, 'name': 'toothbrush'}}\n"
     ]
    }
   ],
   "source": [
    "print(CATEGORY_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, image_dims, batch_size = 32):\n",
    "    import pathlib \n",
    "    import random \n",
    "    print(image_dims)\n",
    "\n",
    "    \n",
    "    data_root = pathlib.Path(path)\n",
    "    image_uris = list(data_root.glob('**/*.jpg'))\n",
    "    image_uris = [str(image) for image in image_uris]\n",
    "    \n",
    "    def preprocess_image(image):\n",
    "\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize_image_with_pad(image, image_dims[0], image_dims[1])\n",
    "        #image /= 255  # normalize to [0,1] range\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def load_and_preprocess_image(path):\n",
    "        \n",
    "        image = tf.read_file(path)\n",
    "        \n",
    "        return preprocess_image(image)\n",
    "    \n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(image_uris)\n",
    "    #AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    image_ds = path_ds.map(load_and_preprocess_image)\n",
    "    \n",
    "    return image_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graph and connect tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inference_graph(inference_graph_path, prepend_name):\n",
    "    \"\"\"Loads the inference graph and connects it to the input image.\n",
    "    Args:\n",
    "    image_tensor: The input image. uint8 tensor, shape=[1, None, None, 3]\n",
    "    inference_graph_path: Path to the inference graph with embedded weights\n",
    "    Returns:\n",
    "    detected_boxes_tensor: Detected boxes. Float tensor,\n",
    "        shape=[num_detections, 4]\n",
    "    detected_scores_tensor: Detected scores. Float tensor,\n",
    "        shape=[num_detections]\n",
    "    detected_labels_tensor: Detected labels. Int64 tensor,\n",
    "        shape=[num_detections]\n",
    "    \"\"\"\n",
    "    with tf.gfile.Open(inference_graph_path, 'rb') as graph_def_file:\n",
    "        graph_content = graph_def_file.read()\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.MergeFromString(graph_content)\n",
    "\n",
    "    tf.import_graph_def(graph_def, name=prepend_name)\n",
    "\n",
    "    g = tf.get_default_graph()\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "    \n",
    "    tensor_dict = {}\n",
    "    for key in ['num_detections', 'detection_boxes', 'detection_scores', 'detection_classes']:\n",
    "        tensor_name = key + ':0'\n",
    "        try:\n",
    "            tensor_dict[key] = g.get_tensor_by_name(prepend_name + '/' + tensor_name)\n",
    "        except:\n",
    "            print(\"Something went horribly wrong when loading graph tensors\")\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    image_tensor = g.get_tensor_by_name(prepend_name + '/' + 'image_tensor:0')\n",
    "    return {'out': tensor_dict, 'in': image_tensor}, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threaded visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "def threaded_function(image_np, args_dict, counter, counter2):\n",
    "    boxes = args_dict['detection_boxes']\n",
    "    scores = args_dict['detection_scores']\n",
    "    classes = np.array(args_dict['detection_classes'], np.int16)\n",
    "    \n",
    "    for i in range(len(image_np)):\n",
    "        new_img = np.array(image_np[i], np.uint8)\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          new_img,\n",
    "          boxes[i],\n",
    "          classes[i],\n",
    "          scores[i],\n",
    "          CATEGORY_INDEX,\n",
    "          use_normalized_coordinates=True,\n",
    "          line_thickness=8)\n",
    "        plt.figure(figsize=FIGURE_DIMS)\n",
    "        plt.imsave(DESTINATION_PATH + '/{}_{}_{}.jpg'.format(str(counter), str(counter2), str(i)), new_img)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def threaded_inference(graph):\n",
    "    #with tf.Session(graph = graph) as sess:\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    #return [x.name for x in local_device_protos if (x.device_type == 'GPU' or x.device_type == 'CPU')]\n",
    "    return [x.name for x in local_device_protos if (x.device_type == 'GPU')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 500]\n",
      "WARNING:tensorflow:From /home/johann/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Reading graph and building model specified for the different cluster devices\n",
      "Step: 1\n",
      "test/image_tensor:0\n",
      "Step: 2\n",
      "test/image_tensor:0\n",
      "Step: 3\n",
      "test/image_tensor:0\n",
      "Step: 4\n",
      "test/image_tensor:0\n",
      "Step: 5\n",
      "test/image_tensor:0\n",
      "Step: 6\n",
      "test/image_tensor:0\n",
      "Step: 7\n",
      "test/image_tensor:0\n",
      "Step: 8\n",
      "test/image_tensor:0\n",
      "Step: 9\n",
      "test/image_tensor:0\n",
      "Step: 10\n",
      "test/image_tensor:0\n",
      "Step: 11\n",
      "test/image_tensor:0\n",
      "Step: 12\n",
      "test/image_tensor:0\n",
      "Step: 13\n",
      "test/image_tensor:0\n",
      "Step: 14\n",
      "test/image_tensor:0\n",
      "Step: 15\n",
      "test/image_tensor:0\n",
      "Step: 16\n",
      "test/image_tensor:0\n",
      "Step: 17\n",
      "test/image_tensor:0\n",
      "Step: 18\n",
      "test/image_tensor:0\n",
      "Step: 19\n",
      "test/image_tensor:0\n",
      "Step: 20\n",
      "test/image_tensor:0\n",
      "Step: 21\n",
      "test/image_tensor:0\n",
      "Step: 22\n",
      "test/image_tensor:0\n",
      "Step: 23\n",
      "test/image_tensor:0\n",
      "Step: 24\n",
      "test/image_tensor:0\n",
      "Step: 25\n",
      "test/image_tensor:0\n",
      "Step: 26\n",
      "test/image_tensor:0\n",
      "Step: 27\n",
      "test/image_tensor:0\n",
      "Step: 28\n",
      "test/image_tensor:0\n",
      "Step: 29\n",
      "test/image_tensor:0\n",
      "Step: 30\n",
      "test/image_tensor:0\n",
      "Step: 31\n",
      "test/image_tensor:0\n",
      "Step: 32\n",
      "test/image_tensor:0\n",
      "Step: 33\n",
      "test/image_tensor:0\n",
      "Step: 34\n",
      "test/image_tensor:0\n",
      "Step: 35\n",
      "test/image_tensor:0\n",
      "Step: 36\n",
      "test/image_tensor:0\n",
      "Step: 37\n",
      "test/image_tensor:0\n",
      "Step: 38\n",
      "test/image_tensor:0\n",
      "Step: 39\n",
      "test/image_tensor:0\n",
      "Step: 40\n",
      "test/image_tensor:0\n",
      "Step: 41\n",
      "test/image_tensor:0\n",
      "Step: 42\n",
      "test/image_tensor:0\n",
      "Step: 43\n",
      "test/image_tensor:0\n",
      "Step: 44\n",
      "test/image_tensor:0\n",
      "Step: 45\n",
      "test/image_tensor:0\n",
      "Step: 46\n",
      "test/image_tensor:0\n",
      "Step: 47\n",
      "test/image_tensor:0\n",
      "Step: 48\n",
      "test/image_tensor:0\n",
      "Step: 49\n",
      "test/image_tensor:0\n",
      "Step: 50\n",
      "test/image_tensor:0\n",
      "Step: 51\n",
      "test/image_tensor:0\n",
      "Step: 52\n",
      "test/image_tensor:0\n",
      "Step: 53\n",
      "test/image_tensor:0\n",
      "Step: 54\n",
      "test/image_tensor:0\n",
      "Step: 55\n",
      "test/image_tensor:0\n",
      "Step: 56\n",
      "test/image_tensor:0\n",
      "Step: 57\n",
      "test/image_tensor:0\n",
      "Step: 58\n",
      "test/image_tensor:0\n",
      "Step: 59\n",
      "test/image_tensor:0\n",
      "Step: 60\n",
      "test/image_tensor:0\n",
      "Step: 61\n",
      "test/image_tensor:0\n",
      "Step: 62\n",
      "test/image_tensor:0\n",
      "Step: 63\n",
      "test/image_tensor:0\n",
      "Step: 64\n",
      "test/image_tensor:0\n",
      "Step: 65\n",
      "test/image_tensor:0\n",
      "Step: 66\n",
      "test/image_tensor:0\n",
      "Step: 67\n",
      "test/image_tensor:0\n",
      "Step: 68\n",
      "test/image_tensor:0\n",
      "Step: 69\n",
      "test/image_tensor:0\n",
      "Step: 70\n",
      "test/image_tensor:0\n",
      "Step: 71\n",
      "test/image_tensor:0\n",
      "Step: 72\n",
      "test/image_tensor:0\n",
      "Step: 73\n",
      "test/image_tensor:0\n",
      "Step: 74\n",
      "test/image_tensor:0\n",
      "Step: 75\n",
      "test/image_tensor:0\n",
      "Step: 76\n",
      "test/image_tensor:0\n",
      "Step: 77\n",
      "test/image_tensor:0\n",
      "Step: 78\n",
      "test/image_tensor:0\n",
      "Step: 79\n",
      "test/image_tensor:0\n",
      "Step: 80\n",
      "test/image_tensor:0\n",
      "Step: 81\n",
      "test/image_tensor:0\n",
      "Step: 82\n",
      "test/image_tensor:0\n",
      "Step: 83\n",
      "test/image_tensor:0\n",
      "Step: 84\n",
      "test/image_tensor:0\n",
      "Step: 85\n",
      "test/image_tensor:0\n",
      "Step: 86\n",
      "test/image_tensor:0\n",
      "Step: 87\n",
      "test/image_tensor:0\n",
      "Step: 88\n",
      "test/image_tensor:0\n",
      "Step: 89\n",
      "test/image_tensor:0\n",
      "Step: 90\n",
      "test/image_tensor:0\n",
      "Step: 91\n",
      "test/image_tensor:0\n",
      "Step: 92\n",
      "test/image_tensor:0\n",
      "Step: 93\n",
      "test/image_tensor:0\n",
      "Step: 94\n",
      "test/image_tensor:0\n",
      "Step: 95\n",
      "test/image_tensor:0\n",
      "Step: 96\n",
      "test/image_tensor:0\n",
      "Step: 97\n",
      "test/image_tensor:0\n",
      "Step: 98\n",
      "test/image_tensor:0\n",
      "Step: 99\n",
      "test/image_tensor:0\n",
      "Step: 100\n",
      "test/image_tensor:0\n",
      "Step: 101\n",
      "test/image_tensor:0\n",
      "Step: 102\n",
      "test/image_tensor:0\n",
      "Step: 103\n",
      "test/image_tensor:0\n",
      "Step: 104\n",
      "test/image_tensor:0\n",
      "Step: 105\n",
      "test/image_tensor:0\n",
      "Step: 106\n",
      "test/image_tensor:0\n",
      "Step: 107\n",
      "test/image_tensor:0\n",
      "Step: 108\n",
      "test/image_tensor:0\n",
      "Step: 109\n",
      "test/image_tensor:0\n",
      "Step: 110\n",
      "test/image_tensor:0\n",
      "Step: 111\n",
      "test/image_tensor:0\n",
      "Step: 112\n",
      "test/image_tensor:0\n",
      "Step: 113\n",
      "test/image_tensor:0\n",
      "Step: 114\n",
      "test/image_tensor:0\n",
      "Step: 115\n",
      "test/image_tensor:0\n",
      "Step: 116\n",
      "test/image_tensor:0\n",
      "Step: 117\n",
      "test/image_tensor:0\n",
      "Step: 118\n",
      "test/image_tensor:0\n",
      "Step: 119\n",
      "test/image_tensor:0\n",
      "Step: 120\n",
      "test/image_tensor:0\n",
      "Step: 121\n",
      "test/image_tensor:0\n",
      "Step: 122\n",
      "test/image_tensor:0\n",
      "Step: 123\n",
      "test/image_tensor:0\n",
      "Step: 124\n",
      "test/image_tensor:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johann/.local/lib/python3.5/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 125\n",
      "test/image_tensor:0\n",
      "Step: 126\n",
      "test/image_tensor:0\n",
      "Step: 127\n",
      "test/image_tensor:0\n",
      "Step: 128\n",
      "test/image_tensor:0\n",
      "Step: 129\n",
      "test/image_tensor:0\n",
      "Step: 130\n",
      "test/image_tensor:0\n",
      "Step: 131\n",
      "test/image_tensor:0\n",
      "Step: 132\n",
      "test/image_tensor:0\n",
      "Step: 133\n",
      "test/image_tensor:0\n",
      "Step: 134\n",
      "test/image_tensor:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-364a1f42b9f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_multi_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;31m#pprint.pprint(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ds = create_dataset(IMAGE_PATH, IMAGE_DIMS)\n",
    "ds = ds.prefetch(BATCH_NUM*2).batch(BATCH_NUM)\n",
    "it = ds.make_one_shot_iterator()\n",
    "\n",
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "#config=tf.ConfigProto(log_device_placement=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #tf.logging.info('Reading input from files and connecting to graph')\n",
    "    #image_tensor = build_input(image_path)\n",
    "    \n",
    "    devices = get_available_gpus()\n",
    "    \n",
    "    tf.logging.info('Reading graph and building model specified for the different cluster devices')\n",
    "    \n",
    "    tf_dict = []\n",
    "    n = 0\n",
    "    for d in devices:\n",
    "        tensors, graph = build_inference_graph(INFERENCE_GRAPH_PATH, 'test')\n",
    "        tf_dict.append(tensors)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    image_it = it.get_next()\n",
    "    \n",
    "    threads = []\n",
    "    \n",
    "    i = 1\n",
    "    j = 1\n",
    "    try:\n",
    "        while True:\n",
    "            print('Step: ' + str(i))\n",
    "            start = time.time()\n",
    "            \n",
    "            output_multi_device = []\n",
    "            input_feed_dict = {}\n",
    "            images = []\n",
    "            \n",
    "            for idx in tf_dict:\n",
    "                \n",
    "                output_multi_device.append(idx['out'])\n",
    "                #print(idx['out'])\n",
    "                loc_image = sess.run(image_it)\n",
    "                print(idx['in'].name)\n",
    "                input_feed_dict[idx['in']] = loc_image\n",
    "                images.append(loc_image)\n",
    "                \n",
    "            outputs = sess.run(output_multi_device, input_feed_dict)\n",
    "            #pprint.pprint(outputs)\n",
    "            \n",
    "            if VISUALIZE:\n",
    "                for count in range(len(tf_dict)):\n",
    "                    thread = Thread(target = threaded_function, args = (images[count], outputs[count], i, count))\n",
    "                    thread.start()\n",
    "                    threads.append(thread)\n",
    "            i += 1\n",
    "                \n",
    "        '''\n",
    "                    image = sess.run(image_it)\n",
    "                    print('Time required for reading: ' + str(time.time() - start))\n",
    "\n",
    "                    start = time.time()\n",
    "                    output_dict = sess.run(tensor_dict, feed_dict = {image_tensor: image})\n",
    "                    print('Time required for computation: ' + str(time.time() - start))\n",
    "            \n",
    "            if VISUALIZE:\n",
    "                thread = Thread(target = threaded_function, args = (image, output_dict, i))\n",
    "                thread.start()\n",
    "                threads.append(thread)\n",
    "                \n",
    "            i += 1\n",
    "        '''\n",
    "    \n",
    "    \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        tf.logging.info('Finished processing records')\n",
    "        for t in threads:\n",
    "            t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dict, image_tensor = build_inference_graph(INFERENCE_GRAPH_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor_dict)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
