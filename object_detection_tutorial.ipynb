{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-yl-s-WKMG"
   },
   "source": [
    "# Object Detection Demo\n",
    "Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFSqkTCdWKMI"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "import pprint\n",
    "\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\".\")\n",
    "#from object_detection.utils import ops as utils_ops\n",
    "\n",
    "#if StrictVersion(tf.__version__) < StrictVersion('1.12.0'):\n",
    "#    raise ImportError('Please upgrade your TensorFlow installation to v1.12.*.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfn_tRFOWKMO"
   },
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference configs\n",
    "IMAGE_PATH = '/home/johann/Pictures/images_compl'\n",
    "DESTINATION_PATH = '/home/johann/Pictures/dest_images'\n",
    "IMAGE_DIMS = [500, 500]\n",
    "FIGURE_DIMS = tuple(IMAGE_DIMS.copy())\n",
    "BATCH_NUM = 20\n",
    "THRESHOLD = 20\n",
    "\n",
    "# What model to download.\n",
    "MODEL_NAME = 'ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "INFERENCE_GRAPH_PATH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "#Download url\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "VISUALIZE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KILYnwR5WKMS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file already exists\n",
      "Model file already extracted\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(MODEL_FILE):\n",
    "    print(\"Downloading model file\")\n",
    "    opener = urllib.request.URLopener()\n",
    "    opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "else:\n",
    "    print(\"Model file already exists\")\n",
    "    \n",
    "if not os.path.isfile(INFERENCE_GRAPH_PATH):\n",
    "    tar_file = tarfile.open(MODEL_FILE)\n",
    "    for file in tar_file.getmembers():\n",
    "        file_name = os.path.basename(file.name)\n",
    "        if 'frozen_inference_graph.pb' in file_name:\n",
    "            tar_file.extract(file, os.getcwd())\n",
    "else:\n",
    "    print('Model file already extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MVVTcLWKMW"
   },
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_INDEX = {}\n",
    "f = open(\"object_detection/mscoco_label_map.pbtxt\", \"r\")\n",
    "for line in f:\n",
    "    if 'item' in line:\n",
    "        name = f.readline().split()[1]\n",
    "        real_id = int(f.readline().split()[1])\n",
    "        real_name = f.readline().split()[1]\n",
    "        CATEGORY_INDEX[real_id] =  {'id': real_id, 'name': real_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'name': 'person', 'id': 1}, 2: {'name': 'bicycle', 'id': 2}, 3: {'name': 'car', 'id': 3}, 4: {'name': 'motorcycle', 'id': 4}, 5: {'name': 'airplane', 'id': 5}, 6: {'name': 'bus', 'id': 6}, 7: {'name': 'train', 'id': 7}, 8: {'name': 'truck', 'id': 8}, 9: {'name': 'boat', 'id': 9}, 10: {'name': 'traffic', 'id': 10}, 11: {'name': 'fire', 'id': 11}, 13: {'name': 'stop', 'id': 13}, 14: {'name': 'parking', 'id': 14}, 15: {'name': 'bench', 'id': 15}, 16: {'name': 'bird', 'id': 16}, 17: {'name': 'cat', 'id': 17}, 18: {'name': 'dog', 'id': 18}, 19: {'name': 'horse', 'id': 19}, 20: {'name': 'sheep', 'id': 20}, 21: {'name': 'cow', 'id': 21}, 22: {'name': 'elephant', 'id': 22}, 23: {'name': 'bear', 'id': 23}, 24: {'name': 'zebra', 'id': 24}, 25: {'name': 'giraffe', 'id': 25}, 27: {'name': 'backpack', 'id': 27}, 28: {'name': 'umbrella', 'id': 28}, 31: {'name': 'handbag', 'id': 31}, 32: {'name': 'tie', 'id': 32}, 33: {'name': 'suitcase', 'id': 33}, 34: {'name': 'frisbee', 'id': 34}, 35: {'name': 'skis', 'id': 35}, 36: {'name': 'snowboard', 'id': 36}, 37: {'name': 'sports', 'id': 37}, 38: {'name': 'kite', 'id': 38}, 39: {'name': 'baseball', 'id': 39}, 40: {'name': 'baseball', 'id': 40}, 41: {'name': 'skateboard', 'id': 41}, 42: {'name': 'surfboard', 'id': 42}, 43: {'name': 'tennis', 'id': 43}, 44: {'name': 'bottle', 'id': 44}, 46: {'name': 'wine', 'id': 46}, 47: {'name': 'cup', 'id': 47}, 48: {'name': 'fork', 'id': 48}, 49: {'name': 'knife', 'id': 49}, 50: {'name': 'spoon', 'id': 50}, 51: {'name': 'bowl', 'id': 51}, 52: {'name': 'banana', 'id': 52}, 53: {'name': 'apple', 'id': 53}, 54: {'name': 'sandwich', 'id': 54}, 55: {'name': 'orange', 'id': 55}, 56: {'name': 'broccoli', 'id': 56}, 57: {'name': 'carrot', 'id': 57}, 58: {'name': 'hot', 'id': 58}, 59: {'name': 'pizza', 'id': 59}, 60: {'name': 'donut', 'id': 60}, 61: {'name': 'cake', 'id': 61}, 62: {'name': 'chair', 'id': 62}, 63: {'name': 'couch', 'id': 63}, 64: {'name': 'potted', 'id': 64}, 65: {'name': 'bed', 'id': 65}, 67: {'name': 'dining', 'id': 67}, 70: {'name': 'toilet', 'id': 70}, 72: {'name': 'tv', 'id': 72}, 73: {'name': 'laptop', 'id': 73}, 74: {'name': 'mouse', 'id': 74}, 75: {'name': 'remote', 'id': 75}, 76: {'name': 'keyboard', 'id': 76}, 77: {'name': 'cell', 'id': 77}, 78: {'name': 'microwave', 'id': 78}, 79: {'name': 'oven', 'id': 79}, 80: {'name': 'toaster', 'id': 80}, 81: {'name': 'sink', 'id': 81}, 82: {'name': 'refrigerator', 'id': 82}, 84: {'name': 'book', 'id': 84}, 85: {'name': 'clock', 'id': 85}, 86: {'name': 'vase', 'id': 86}, 87: {'name': 'scissors', 'id': 87}, 88: {'name': 'teddy', 'id': 88}, 89: {'name': 'hair', 'id': 89}, 90: {'name': 'toothbrush', 'id': 90}}\n"
     ]
    }
   ],
   "source": [
    "print(CATEGORY_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, image_dims, batch_size = 32):\n",
    "    import pathlib \n",
    "    import random \n",
    "    print(image_dims)\n",
    "\n",
    "    \n",
    "    data_root = pathlib.Path(path)\n",
    "    image_uris = list(data_root.glob('**/*.jpg'))\n",
    "    image_uris = [str(image) for image in image_uris]\n",
    "    \n",
    "    def preprocess_image(image):\n",
    "\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize_image_with_pad(image, image_dims[0], image_dims[1])\n",
    "        #image /= 255  # normalize to [0,1] range\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def load_and_preprocess_image(path):\n",
    "        \n",
    "        image = tf.read_file(path)\n",
    "        \n",
    "        return preprocess_image(image)\n",
    "    \n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(image_uris)\n",
    "    #AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    image_ds = path_ds.map(load_and_preprocess_image)\n",
    "    \n",
    "    return image_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graph and connect tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inference_graph(inference_graph_path, prepend_name):\n",
    "    \"\"\"Loads the inference graph and connects it to the input image.\n",
    "    Args:\n",
    "    image_tensor: The input image. uint8 tensor, shape=[1, None, None, 3]\n",
    "    inference_graph_path: Path to the inference graph with embedded weights\n",
    "    Returns:\n",
    "    detected_boxes_tensor: Detected boxes. Float tensor,\n",
    "        shape=[num_detections, 4]\n",
    "    detected_scores_tensor: Detected scores. Float tensor,\n",
    "        shape=[num_detections]\n",
    "    detected_labels_tensor: Detected labels. Int64 tensor,\n",
    "        shape=[num_detections]\n",
    "    \"\"\"\n",
    "    with tf.gfile.Open(inference_graph_path, 'rb') as graph_def_file:\n",
    "        graph_content = graph_def_file.read()\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.MergeFromString(graph_content)\n",
    "\n",
    "    tf.import_graph_def(graph_def, name=prepend_name)\n",
    "\n",
    "    g = tf.get_default_graph()\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "    \n",
    "    tensor_dict = {}\n",
    "    for key in ['num_detections', 'detection_boxes', 'detection_scores', 'detection_classes']:\n",
    "        tensor_name = key + ':0'\n",
    "        try:\n",
    "            tensor_dict[key] = g.get_tensor_by_name(prepend_name + '/' + tensor_name)\n",
    "        except:\n",
    "            print(\"Something went horribly wrong when loading graph tensors\")\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    image_tensor = g.get_tensor_by_name(prepend_name + '/' + 'image_tensor:0')\n",
    "    return {'out': tensor_dict, 'in': image_tensor}, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threaded visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "vis_threads = []\n",
    "\n",
    "def threaded_function(image_np, args_dict, counter):\n",
    "    boxes = args_dict['detection_boxes']\n",
    "    scores = args_dict['detection_scores']\n",
    "    classes = np.array(args_dict['detection_classes'], np.int16)\n",
    "    \n",
    "    for i in range(len(image_np)):\n",
    "        new_img = np.array(image_np[i], np.uint8)\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          new_img,\n",
    "          boxes[i],\n",
    "          classes[i],\n",
    "          scores[i],\n",
    "          CATEGORY_INDEX,\n",
    "          use_normalized_coordinates=True,\n",
    "          line_thickness=8)\n",
    "        plt.figure(figsize=FIGURE_DIMS)\n",
    "        plt.imsave(DESTINATION_PATH + '/{}_{}.jpg'.format(str(counter), str(i)), new_img)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threaded_inference(tensor_dict, image_tensor, graph, image, counter):\n",
    "    config=tf.ConfigProto(log_device_placement=True)\n",
    "    with tf.Session(graph = graph, config = config) as sess:\n",
    "        output_dict = sess.run(tensor_dict, feed_dict = {image_tensor: image})\n",
    "        \n",
    "        if VISUALIZE:\n",
    "                thread = Thread(target = threaded_function, args = (image, output_dict, counter))\n",
    "                thread.start()\n",
    "                vis_threads.append(thread)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if (x.device_type == 'GPU' or x.device_type == 'CPU')]\n",
    "    #return [x.name for x in local_device_protos if (x.device_type == 'GPU')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 500]\n",
      "WARNING:tensorflow:From /home/johann/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Reading graph and building model specified for the different cluster devices\n",
      "Step: 1\n",
      "Step: 3\n",
      "Step: 5\n",
      "Step: 7\n",
      "Step: 9\n",
      "Step: 11\n",
      "Step: 13\n",
      "Step: 15\n",
      "Step: 17\n",
      "Step: 19\n",
      "Step: 21\n",
      "Step: 23\n",
      "Step: 25\n",
      "Step: 27\n",
      "Step: 29\n",
      "Step: 31\n",
      "Step: 33\n",
      "Step: 35\n",
      "Step: 37\n",
      "Step: 39\n",
      "Step: 41\n",
      "Step: 43\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d983e5026421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         '''\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ds = create_dataset(IMAGE_PATH, IMAGE_DIMS)\n",
    "ds = ds.prefetch(BATCH_NUM*2).batch(BATCH_NUM)\n",
    "it = ds.make_one_shot_iterator()\n",
    "\n",
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "#config=tf.ConfigProto(log_device_placement=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #tf.logging.info('Reading input from files and connecting to graph')\n",
    "    #image_tensor = build_input(image_path)\n",
    "    \n",
    "    devices = get_available_gpus()\n",
    "    \n",
    "    tf.logging.info('Reading graph and building model specified for the different cluster devices')\n",
    "    \n",
    "    tf_dict = []\n",
    "    tg_dict = []\n",
    "    n = 0\n",
    "    for d in devices:\n",
    "        tensors, graph = build_inference_graph(INFERENCE_GRAPH_PATH, 'test')\n",
    "        tf_dict.append(tensors)\n",
    "        tg_dict.append(graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    image_it = it.get_next()\n",
    "    \n",
    "    #threads = []\n",
    "    \n",
    "    i = 1\n",
    "    j = 1\n",
    "    try:\n",
    "        while True:\n",
    "            threads = []\n",
    "            print('Step: ' + str(i))\n",
    "            for idx in range(len(tf_dict)):\n",
    "                loc_image = sess.run(image_it)\n",
    "                thread = Thread(target = threaded_inference, \n",
    "                                args = (tf_dict[idx]['out'], tf_dict[idx]['in'], tg_dict[idx], loc_image, i))\n",
    "                i += 1\n",
    "                thread.start()\n",
    "                threads.append(thread)\n",
    "            \n",
    "            for t in threads:\n",
    "                t.join()\n",
    "        '''\n",
    "        while True:\n",
    "            print('Step: ' + str(i))\n",
    "            start = time.time()\n",
    "            \n",
    "            output_multi_device = []\n",
    "            input_feed_dict = {}\n",
    "            images = []\n",
    "            \n",
    "            for idx in tf_dict:\n",
    "                \n",
    "                output_multi_device.append(idx['out'])\n",
    "                #print(idx['out'])\n",
    "                loc_image = sess.run(image_it)\n",
    "                print(idx['in'].name)\n",
    "                input_feed_dict[idx['in']] = loc_image\n",
    "                images.append(loc_image)\n",
    "                \n",
    "            outputs = sess.run(output_multi_device, input_feed_dict)\n",
    "            #pprint.pprint(outputs)\n",
    "            \n",
    "            if VISUALIZE:\n",
    "                for count in range(len(tf_dict)):\n",
    "                    thread = Thread(target = threaded_function, args = (images[count], outputs[count], i, count))\n",
    "                    thread.start()\n",
    "                    threads.append(thread)\n",
    "            i += 1\n",
    "                \n",
    "                    image = sess.run(image_it)\n",
    "                    print('Time required for reading: ' + str(time.time() - start))\n",
    "\n",
    "                    start = time.time()\n",
    "                    output_dict = sess.run(tensor_dict, feed_dict = {image_tensor: image})\n",
    "                    print('Time required for computation: ' + str(time.time() - start))\n",
    "            \n",
    "            if VISUALIZE:\n",
    "                thread = Thread(target = threaded_function, args = (image, output_dict, i))\n",
    "                thread.start()\n",
    "                threads.append(thread)\n",
    "                \n",
    "            i += 1\n",
    "        '''\n",
    "    \n",
    "    \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        tf.logging.info('Finished processing records')\n",
    "        for t in vis_threads:\n",
    "            t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dict, image_tensor = build_inference_graph(INFERENCE_GRAPH_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor_dict)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
